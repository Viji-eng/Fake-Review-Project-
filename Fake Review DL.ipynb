{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a0eb47-b68e-4770-a32c-08faae3ce1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, SimpleRNN\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9603b7ee-2f07-4673-829e-0fa9d6ed3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and then encode the labels into numeric values using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6b997b-c697-4cc1-8dab-10cac4867567",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Text_pre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7217b6-0708-4ee3-8922-dec021c80f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6949e881-2dc6-4be3-8f92-0ea74a8e9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the data into training and testing sets\n",
    "X = data['cleaned_text']\n",
    "y = data['label_encoded']\n",
    "X = data['cleaned_text'].astype(str)  # Convert to string to handle any float or NaN issues\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0650838-cc17-4124-847c-49c0901eef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Text Preprocessing with Tokenization and Padding\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a894d0-05d1-4fc2-aba8-53fc67c9e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 111ms/step - accuracy: 0.5798 - loss: 0.6671 - precision: 0.6838 - recall: 0.2903 - val_accuracy: 0.6738 - val_loss: 0.6248 - val_precision: 0.7454 - val_recall: 0.5319\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 122ms/step - accuracy: 0.6679 - loss: 0.6298 - precision: 0.7034 - recall: 0.5728 - val_accuracy: 0.6802 - val_loss: 0.6003 - val_precision: 0.7110 - val_recall: 0.6113\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 116ms/step - accuracy: 0.7818 - loss: 0.4813 - precision: 0.8395 - recall: 0.6975 - val_accuracy: 0.8621 - val_loss: 0.3384 - val_precision: 0.8945 - val_recall: 0.8223\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 109ms/step - accuracy: 0.8873 - loss: 0.2739 - precision: 0.9016 - recall: 0.8704 - val_accuracy: 0.9009 - val_loss: 0.2392 - val_precision: 0.8934 - val_recall: 0.9113\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 116ms/step - accuracy: 0.9312 - loss: 0.1779 - precision: 0.9325 - recall: 0.9279 - val_accuracy: 0.9026 - val_loss: 0.2383 - val_precision: 0.8882 - val_recall: 0.9221\n",
      "Epoch 6/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 117ms/step - accuracy: 0.9478 - loss: 0.1383 - precision: 0.9483 - recall: 0.9463 - val_accuracy: 0.9052 - val_loss: 0.2287 - val_precision: 0.9105 - val_recall: 0.8996\n",
      "Epoch 7/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 111ms/step - accuracy: 0.9597 - loss: 0.1093 - precision: 0.9629 - recall: 0.9552 - val_accuracy: 0.8964 - val_loss: 0.2674 - val_precision: 0.8806 - val_recall: 0.9181\n",
      "Epoch 8/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 108ms/step - accuracy: 0.9673 - loss: 0.0903 - precision: 0.9686 - recall: 0.9654 - val_accuracy: 0.8953 - val_loss: 0.2865 - val_precision: 0.9026 - val_recall: 0.8873\n",
      "Epoch 9/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 115ms/step - accuracy: 0.9739 - loss: 0.0745 - precision: 0.9776 - recall: 0.9689 - val_accuracy: 0.8981 - val_loss: 0.3052 - val_precision: 0.8879 - val_recall: 0.9122\n",
      "\u001b[1m253/253\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step\n",
      "Classification Report for LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.89      0.91      0.90      4016\n",
      "          OR       0.91      0.89      0.90      4071\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 187ms/step - accuracy: 0.7694 - loss: 0.4393 - precision_1: 0.8079 - recall_1: 0.6773 - val_accuracy: 0.8983 - val_loss: 0.2343 - val_precision_1: 0.8712 - val_recall_1: 0.9356\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 168ms/step - accuracy: 0.9266 - loss: 0.1847 - precision_1: 0.9240 - recall_1: 0.9295 - val_accuracy: 0.8938 - val_loss: 0.2472 - val_precision_1: 0.8428 - val_recall_1: 0.9692\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 156ms/step - accuracy: 0.9413 - loss: 0.1424 - precision_1: 0.9392 - recall_1: 0.9429 - val_accuracy: 0.9124 - val_loss: 0.2108 - val_precision_1: 0.9143 - val_recall_1: 0.9107\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 147ms/step - accuracy: 0.9644 - loss: 0.0916 - precision_1: 0.9661 - recall_1: 0.9621 - val_accuracy: 0.9136 - val_loss: 0.2396 - val_precision_1: 0.9153 - val_recall_1: 0.9122\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 145ms/step - accuracy: 0.9731 - loss: 0.0715 - precision_1: 0.9751 - recall_1: 0.9709 - val_accuracy: 0.9085 - val_loss: 0.2651 - val_precision_1: 0.9189 - val_recall_1: 0.8968\n",
      "Epoch 6/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 148ms/step - accuracy: 0.9774 - loss: 0.0582 - precision_1: 0.9778 - recall_1: 0.9772 - val_accuracy: 0.9063 - val_loss: 0.3055 - val_precision_1: 0.9074 - val_recall_1: 0.9058\n",
      "\u001b[1m253/253\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step\n",
      "Classification Report for BiLSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.91      0.90      0.91      4016\n",
      "          OR       0.91      0.91      0.91      4071\n",
      "\n",
      "    accuracy                           0.91      8087\n",
      "   macro avg       0.91      0.91      0.91      8087\n",
      "weighted avg       0.91      0.91      0.91      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Model building function\n",
    "def build_model(model_type=\"LSTM\", vocab_size=10000, embedding_dim=100, max_length=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    if model_type == \"BiLSTM\":\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    else:\n",
    "        model.add(LSTM(64, return_sequences=False))  # Default to LSTM if not BiLSTM\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model_type, X_train_pad, y_train, X_test_pad, y_test):\n",
    "    model = build_model(model_type)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train_pad, y_train, \n",
    "                        epochs=10, \n",
    "                        batch_size=64, \n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluation of the model\n",
    "    y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"Classification Report for {model_type}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['CG', 'OR']))\n",
    "\n",
    "# Loop through and train and evaluate LSTM and BiLSTM models\n",
    "# Assuming X_train_pad, y_train, X_test_pad, y_test are already defined\n",
    "for model_type in [\"LSTM\", \"BiLSTM\"]:\n",
    "    train_and_evaluate(model_type, X_train_pad, y_train, X_test_pad, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79ce4cf-d55e-4687-8642-ee6adc12016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training, Prediction, and Evaluation Using BERT for Sequence Classification\n",
    "#fine-tune a BERT model for sequence classification using the Transformers library by Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae04c23-9109-4803-8492-56266c7bc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb44d09-db3d-4e9d-b444-9551c15b1148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd26606-6ac2-41fa-aee0-00cbbb28b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1fc1cc-1599-4bb4-bcc9-f31c5c9e77c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af5bde4-16c4-4ae6-8a1f-c14bb60f13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b271fb25-3206-4bec-8bb6-64dd2051783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6024\\3332937762.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16174' max='16174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16174/16174 29:26:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.385804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.357262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.89      0.95      0.92      4016\n",
      "          OR       0.94      0.89      0.91      4071\n",
      "\n",
      "    accuracy                           0.92      8087\n",
      "   macro avg       0.92      0.92      0.92      8087\n",
      "weighted avg       0.92      0.92      0.92      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used (CPU or GPU)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset and preprocess\n",
    "data = pd.read_csv('Text_pre.csv')  # Replace with your file path\n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])\n",
    "\n",
    "# Split dataset\n",
    "X = data['cleaned_text'].astype(str)\n",
    "y = data['label_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class to handle text and labels\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().to(device),\n",
    "            'attention_mask': encoding['attention_mask'].flatten().to(device),\n",
    "            'labels': torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }\n",
    "\n",
    "# Create dataset and data loaders\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ReviewDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',# Adjust evaluation strategy (can set to None to avoid evaluation)\n",
    "    per_device_train_batch_size=4,# \n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,  # Consider reducing epochs for faster traini\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Define Trainer for training BERT model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1).numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_labels, target_names=['CG', 'OR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f932a2-7083-4bca-8c7b-f056fbb0b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and training arguments saved in BERT./saved_model\n"
     ]
    }
   ],
   "source": [
    "## saving  trained model, tokenizer, and training arguments\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define a directory to save the model and tokenizer\n",
    "model_dir = 'BERT./saved_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained BERT model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# Save training arguments as a JSON file\n",
    "with open(os.path.join(model_dir, 'training_args.json'), 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "print(f\"Model, tokenizer, and training arguments saved in {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06517089-d88a-4851-b541-e405c5cc0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion:\n",
    "The model is highly effective at detecting fake reviews with strong performance across both classes. With an accuracy of 92%, it strikes a good balance between precision and recall, showing that it is both accurate and reliable in classifying reviews as genuine or fake. This result is promising for the task of fake review detection.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
